#!/usr/bin/env python3

"""
Transfert de Style Neuronal HTML et PDF - Syst√®me Avanc√©
Extraction et application intelligente des styles visuels entre pages web et documents PDF.
"""

# --- Imports Python standard ---
import asyncio
import json
import re
import io
import os  # Ajout pour les variables d'environnement
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
import logging
from pathlib import Path
import base64
import hashlib
import colorsys
from urllib.parse import urljoin, urlparse
import math

# --- Streamlit (optionnel selon le contexte) ---
try:
    import streamlit as st
    STREAMLIT_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è  Streamlit non disponible - utilisation des variables d'environnement")
    STREAMLIT_AVAILABLE = False

# --- Biblioth√®ques d'analyse web (obligatoires) ---
try:
    import aiohttp
    from bs4 import BeautifulSoup  # CORRECTION: bs4, pas beautifulsoup4
    import tinycss2
    import webcolors
    WEB_ANALYSIS_AVAILABLE = True
except ImportError as e:
    print(f"‚ùå D√©pendances web manquantes: {e}")
    print("Installez avec: pip install aiohttp beautifulsoup4 tinycss2 webcolors")
    WEB_ANALYSIS_AVAILABLE = False
    # D√©finir comme None pour √©viter les erreurs
    aiohttp = None
    BeautifulSoup = None
    tinycss2 = None
    webcolors = None

# --- Traitement PDF (optionnel) ---
try:
    import fitz  # PyMuPDF
    from PIL import Image
    PDF_PROCESSING_AVAILABLE = True
    print("‚úÖ PyMuPDF et Pillow disponibles")
except ImportError as e:
    print(f"‚ö†Ô∏è  Traitement PDF non disponible: {e}")
    print("Pour le support PDF: pip install PyMuPDF Pillow")
    PDF_PROCESSING_AVAILABLE = False
    fitz = None
    Image = None

# --- IA et analyse (optionnel) ---
try:
    import openai
    AI_OPENAI_AVAILABLE = True
    print("‚úÖ OpenAI disponible")
except ImportError:
    print("‚ö†Ô∏è  OpenAI non disponible")
    AI_OPENAI_AVAILABLE = False
    openai = None

try:
    from anthropic import AsyncAnthropic
    AI_ANTHROPIC_AVAILABLE = True
    print("‚úÖ Anthropic disponible")
except ImportError:
    print("‚ö†Ô∏è  Anthropic non disponible")
    AI_ANTHROPIC_AVAILABLE = False
    AsyncAnthropic = None

try:
    import numpy as np
    from sklearn.cluster import KMeans
    ML_AVAILABLE = True
    print("‚úÖ Numpy et scikit-learn disponibles")
except ImportError as e:
    print(f"‚ö†Ô∏è  Machine Learning non disponible: {e}")
    print("Pour le clustering: pip install numpy scikit-learn")
    ML_AVAILABLE = False
    np = None
    KMeans = None

# --- Configuration du Logging ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- V√©rification des d√©pendances critiques au d√©marrage ---
def check_critical_dependencies():
    """V√©rifie les d√©pendances critiques et affiche le statut."""
    print("\nüîç V√©rification des d√©pendances:")
    
    critical_missing = []
    if not WEB_ANALYSIS_AVAILABLE:
        critical_missing.append("Analyse web (aiohttp, beautifulsoup4, tinycss2, webcolors)")
    
    optional_missing = []
    if not PDF_PROCESSING_AVAILABLE:
        optional_missing.append("Traitement PDF (PyMuPDF, Pillow)")
    if not AI_OPENAI_AVAILABLE:
        optional_missing.append("OpenAI")
    if not AI_ANTHROPIC_AVAILABLE:
        optional_missing.append("Anthropic")
    if not ML_AVAILABLE:
        optional_missing.append("Machine Learning (numpy, scikit-learn)")
    if not STREAMLIT_AVAILABLE:
        optional_missing.append("Streamlit")
    
    if critical_missing:
        print("‚ùå D√©pendances CRITIQUES manquantes:")
        for dep in critical_missing:
            print(f"   - {dep}")
        return False
    
    print("‚úÖ Toutes les d√©pendances critiques sont disponibles")
    
    if optional_missing:
        print("‚ö†Ô∏è  D√©pendances optionnelles manquantes:")
        for dep in optional_missing:
            print(f"   - {dep}")
    
    return True

# --- Configuration des cl√©s API ---
def get_ai_config() -> Dict[str, str]:
    """R√©cup√®re la configuration des cl√©s API selon le contexte."""
    
    if STREAMLIT_AVAILABLE and hasattr(st, 'secrets'):
        # Contexte Streamlit
        try:
            return {
                'openai_api_key': st.secrets.get("openai_api_key", ""),
                'anthropic_api_key': st.secrets.get("anthropic_api_key", "")
            }
        except Exception as e:
            logger.warning(f"Impossible de lire les secrets Streamlit: {e}")
    
    # Contexte standalone - utiliser variables d'environnement
    return {
        'openai_api_key': os.getenv('OPENAI_API_KEY', ''),
        'anthropic_api_key': os.getenv('ANTHROPIC_API_KEY', '')
    }

# --- Instructions d'installation ---
INSTALLATION_INSTRUCTIONS = """
üì¶ Instructions d'installation compl√®te:

# D√©pendances critiques (obligatoires)
pip install aiohttp beautifulsoup4 tinycss2 webcolors

# D√©pendances optionnelles
pip install PyMuPDF Pillow                    # Support PDF
pip install openai anthropic                  # IA
pip install numpy scikit-learn               # Machine Learning
pip install streamlit                         # Interface web

# Installation compl√®te
pip install aiohttp beautifulsoup4 tinycss2 webcolors PyMuPDF Pillow openai anthropic numpy scikit-learn streamlit

üîë Configuration des cl√©s API:

# Variables d'environnement (recommand√©)
export OPENAI_API_KEY="sk-..."
export ANTHROPIC_API_KEY="sk-ant-..."

# Ou dans Streamlit secrets.toml
openai_api_key = "sk-..."
anthropic_api_key = "sk-ant-..."
"""

# Affichage des instructions si des d√©pendances manquent
if __name__ == "__main__":
    if not check_critical_dependencies():
        print(INSTALLATION_INSTRUCTIONS)
        exit(1)

# Traitement PDF (PyMuPDF)
try:
    import fitz  # PyMuPDF
    from PIL import Image
except ImportError:
    print("PyMuPDF et Pillow sont requis pour le traitement PDF. Installez-les avec 'pip install PyMuPDF Pillow'")
    fitz = None
    Image = None

# IA et analyse
try:
    import openai
    from anthropic import AsyncAnthropic  # CORRECTION 20: Import√© mais peut √™tre optionnel
    import numpy as np
    from sklearn.cluster import KMeans
except ImportError:
    print("Les biblioth√®ques d'IA sont requises. Installez-les avec 'pip install openai anthropic numpy scikit-learn'")
    openai = None
    AsyncAnthropic = None
    np = None
    KMeans = None

# --- Configuration du Logging ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- Structures de Donn√©es ---

@dataclass
class StyleFingerprint:
    """Empreinte stylistique d'une page web ou d'un document PDF."""
    color_palette: List[str]
    typography: Dict[str, Any]
    layout_patterns: Dict[str, Any]
    spacing_system: Dict[str, Any]
    visual_hierarchy: Dict[str, Any]
    branding_elements: Dict[str, Any]
    responsive_breakpoints: List[int]
    css_rules: Dict[str, Any]
    metadata_profile: Dict[str, Any]
    design_mood: str
    confidence_score: float
    pdf_text_content: Optional[str] = None
    pdf_page_count: Optional[int] = None
    pdf_image_count: Optional[int] = None

# --- Classe Principale d'Analyse ---

class WebStyleAnalyzer:
    """Analyseur de styles web et de documents PDF avec IA."""

    # CORRECTION 2, 17: Utilisation de cha√Ænes raw (r'') pour toutes les expressions r√©guli√®res
    # Patterns pour les couleurs
    _COLOR_HEX_PATTERN = re.compile(r'#([A-Fa-f0-9]{6}|[A-Fa-f0-9]{3})\b')
    _COLOR_RGB_PATTERN = re.compile(r'rgb\(\s*(\d{1,3})\s*,\s*(\d{1,3})\s*,\s*(\d{1,3})\s*\)')
    _COLOR_RGBA_PATTERN = re.compile(r'rgba\(\s*(\d{1,3})\s*,\s*(\d{1,3})\s*,\s*(\d{1,3})\s*,\s*([0-9.]+)\s*\)')
    _COLOR_HSL_PATTERN = re.compile(r'hsl\(\s*(\d+)\s*,\s*([\d.]+%)\s*,\s*([\d.]+%)\s*\)')
    _COLOR_HSLA_PATTERN = re.compile(r'hsla\(\s*(\d+)\s*,\s*([\d.]+%)\s*,\s*([\d.]+%)\s*,\s*([0-9.]+)\s*\)')
    _COLOR_KEYWORDS_PATTERN = re.compile(r'\b(transparent|currentColor|inherit|initial|unset|revert)\b', re.IGNORECASE)
    # CORRECTION 1, 4, 7: Utilisation de l'attribut correct `CSS3_HEX_TO_NAMES` (majuscules)
    _COLOR_NAMES_PATTERN = re.compile(r'\b(' + '|'.join([webcolors.name_to_hex(name, spec='css3') for name in webcolors.names("css3")]
) + r')\b', re.IGNORECASE)

    # Patterns pour la typographie
    _FONT_FAMILY_PATTERN = re.compile(r'font-family\s*:\s*([^;]+)', re.IGNORECASE)
    _FONT_SIZE_PATTERN = re.compile(r'font-size\s*:\s*([^;]+)', re.IGNORECASE)
    _FONT_WEIGHT_PATTERN = re.compile(r'font-weight\s*:\s*([^;]+)', re.IGNORECASE)
    _LINE_HEIGHT_PATTERN = re.compile(r'line-height\s*:\s*([^;]+)', re.IGNORECASE)

    # Patterns pour l'espacement
    _MARGIN_PATTERN = re.compile(r'margin[^:]*:\s*([^;]+)', re.IGNORECASE)
    _PADDING_PATTERN = re.compile(r'padding[^:]*:\s*([^;]+)', re.IGNORECASE)
    _NUMERIC_SPACING_PATTERN = re.compile(r'(-?\d+(?:\.\d+)?)(px|em|rem|%|vw|vh|pt)?')
    _UTILITY_SPACING_CLASS_PATTERN = re.compile(r'^(m|p)[tbrlxy]?-(\d+(?:\.\d+)?|auto)$', re.IGNORECASE)
    _DEFAULT_SPACING_SCALE = [8.0, 16.0, 24.0, 32.0, 48.0]

    # Patterns pour les classes CSS √† exclure
    _EXCLUDED_CLASS_PATTERNS = [
        re.compile(p, re.IGNORECASE) for p in [
            r'^col-', r'^row', r'^container', r'^d-flex', r'^justify-content-', r'^align-items-',
            r'^(m|p)[tbrlxy]?-', r'^(w|h)-', r'^text-', r'^bg-', r'^font-', r'^border-',
            r'^shadow-', r'clearfix', r'float-', r'sr-only', r'^grid', r'^flex',
            r'active', r'selected', r'hidden', r'visible', r'item', r'list',
            r'wrapper', r'content', r'block', r'nav', r'header', r'footer'
        ]
    ]

    def __init__(self, ai_config: Dict[str, str]):
        # CORRECTION 11: V√©rification plus robuste des cl√©s API
        openai_key = ai_config.get('openai_api_key')
        anthropic_key = ai_config.get('anthropic_api_key')

        if not openai_key or not openai_key.startswith('sk-'):
            logger.warning("Cl√© API OpenAI invalide ou manquante.")
            self.openai_client = None
        else:
            self.openai_client = openai.AsyncOpenAI(api_key=openai_key)

        if not anthropic_key or not anthropic_key.startswith('sk-ant-'):
            logger.warning("Cl√© API Anthropic invalide ou manquante.")
            self.anthropic_client = None
        else:
            self.anthropic_client = AsyncAnthropic(api_key=anthropic_key)

        self.session = None

    async def __aenter__(self):
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            headers={'User-Agent': 'Mozilla/5.0 (compatible; StyleAnalyzer/1.0; +http://example.com/bot)'}
        )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        # CORRECTION 27: V√©rification si la session est d√©j√† ferm√©e
        if self.session and not self.session.closed:
            await self.session.close()

    async def analyze_reference_page(self, source_input: str, is_url: bool = True, is_pdf: bool = False) -> StyleFingerprint:
        """
        Analyse compl√®te d'une page de r√©f√©rence ou d'un document PDF.
        """
        # CORRECTION 21: Validation des param√®tres d'entr√©e contradictoires
        if is_pdf and not (fitz and Image and np and KMeans):
            raise ImportError("Les d√©pendances pour l'analyse PDF ne sont pas install√©es.")
        if not source_input:
            raise ValueError("source_input ne peut pas √™tre vide.")

        logger.info(f"D√©but de l'analyse de la source: {'URL' if is_url else 'Contenu'}{' PDF' if is_pdf else ' HTML'}")

        if is_pdf:
            return await self._analyze_pdf_source(source_input, is_url)
        else:
            return await self._analyze_html_source(source_input, is_url)

    async def _analyze_html_source(self, source_input: str, is_url: bool) -> StyleFingerprint:
        """Logique d'analyse pour les sources HTML."""
        if is_url:
            html_content, page_resources = await self._fetch_page_with_resources(source_input)
        else:
            html_content = source_input
            # Pour le contenu local, nous ne pouvons pas r√©cup√©rer les ressources externes
            page_resources = {'css': [], 'js': [], 'images': []}

        soup = BeautifulSoup(html_content, 'html.parser')
        all_css_content_list = self._get_all_css_content(soup, page_resources)

        # Ex√©cution des t√¢ches d'analyse en parall√®le
        analysis_tasks = await asyncio.gather(
            self._extract_color_palette(all_css_content_list, soup),
            self._analyze_typography(all_css_content_list, soup),
            self._extract_layout_patterns(soup),
            self._analyze_spacing_system(all_css_content_list, soup),
            self._detect_visual_hierarchy(soup),
            self._extract_branding_elements(soup),
            self._analyze_responsive_design(all_css_content_list),
            self._extract_metadata_profile(soup)
        )
        color_palette, typography, layout, spacing, hierarchy, branding, responsive, metadata = analysis_tasks

        css_rules = self._extract_css_rules(all_css_content_list)
        design_mood = await self._analyze_design_mood_with_ai(soup, color_palette, typography)

        analysis_data = {
            'color_palette': color_palette, 'typography': typography, 'layout_patterns': layout,
            'spacing_system': spacing, 'visual_hierarchy': hierarchy, 'branding_elements': branding,
            'responsive_breakpoints': responsive, 'css_rules': css_rules, 'metadata_profile': metadata
        }
        confidence = self._calculate_analysis_confidence(analysis_data)

        return StyleFingerprint(
            **analysis_data,
            design_mood=design_mood,
            confidence_score=confidence
        )

    async def _analyze_pdf_source(self, source_input: str, is_url: bool) -> StyleFingerprint:
        """Logique d'analyse pour les sources PDF."""
        pdf_data = None
        if is_url:
            try:
                # CORRECTION 22: Timeout sp√©cifique pour la lecture
                async with self.session.get(source_input, timeout=aiohttp.ClientTimeout(total=60)) as response:
                    response.raise_for_status()
                    pdf_data = await response.read()
            except (aiohttp.ClientError, asyncio.TimeoutError) as e:
                logger.error(f"Erreur lors du t√©l√©chargement du PDF {source_input}: {e}")
                raise ValueError(f"Impossible de t√©l√©charger le PDF.") from e
        else:
            try:
                with open(source_input, 'rb') as f:
                    pdf_data = f.read()
            except (FileNotFoundError, IOError) as e:
                logger.error(f"Erreur de lecture du fichier PDF {source_input}: {e}")
                raise ValueError("Fichier PDF non trouv√© ou illisible.") from e

        # CORRECTION 34: Validation du contenu PDF (magic number)
        if not pdf_data or not pdf_data.startswith(b'%PDF-'):
            raise ValueError("La source fournie n'est pas un fichier PDF valide.")

        pdf_analysis = await self._analyze_pdf_document(pdf_data)

        confidence = self._calculate_pdf_analysis_confidence(pdf_analysis)

        return StyleFingerprint(
            color_palette=pdf_analysis['color_palette'],
            typography=pdf_analysis['typography'],
            pdf_text_content=pdf_analysis['text_content'],
            pdf_page_count=pdf_analysis['page_count'],
            pdf_image_count=pdf_analysis['image_count'],
            confidence_score=confidence,
            # Valeurs par d√©faut pour les champs non applicables au PDF
            layout_patterns={'type': 'PDF Document'},
            spacing_system={'type': 'N/A'},
            visual_hierarchy={'type': 'N/A'},
            branding_elements={'type': 'N/A'},
            responsive_breakpoints=[],
            css_rules={},
            metadata_profile={'source_type': 'PDF'},
            design_mood='document'
        )

    def _get_all_css_content(self, soup: BeautifulSoup, resources: Dict[str, Any]) -> List[str]:
        """Collecte tout le contenu CSS (inline <style> et externe)."""
        all_css = [css_res['content'] for css_res in resources.get('css', [])]
        all_css.extend(style.string for style in soup.find_all('style') if style.string)
        return all_css

    async def _fetch_page_with_resources(self, url: str) -> Tuple[str, Dict[str, Any]]:
        """R√©cup√®re la page et ses ressources CSS."""
        # CORRECTION 15: Validation plus robuste de l'URL de base
        parsed_url = urlparse(url)
        if not all([parsed_url.scheme, parsed_url.netloc]):
            raise ValueError(f"URL de base invalide : {url}")

        try:
            async with self.session.get(url) as response:
                response.raise_for_status()
                html_content = await response.text(encoding='utf-8', errors='replace')
        except (aiohttp.ClientError, asyncio.TimeoutError) as e:
            logger.error(f"Impossible de r√©cup√©rer la page principale {url}: {e}")
            raise ValueError(f"Erreur de r√©cup√©ration de la page principale.") from e

        soup = BeautifulSoup(html_content, 'html.parser')
        css_links = [link.get('href') for link in soup.find_all('link', rel='stylesheet') if link.get('href')]

        # CORRECTION 38: Limitation du nombre de ressources CSS √† t√©l√©charger
        MAX_CSS_FILES = 20
        tasks = []
        for href in css_links[:MAX_CSS_FILES]:
            css_url = urljoin(url, href)
            if urlparse(css_url).scheme in ['http', 'https']:
                tasks.append(self._fetch_resource(css_url))

        css_results = await asyncio.gather(*tasks)
        resources = {'css': [res for res in css_results if res], 'js': [], 'images': []}
        return html_content, resources

    async def _fetch_resource(self, url: str) -> Optional[Dict[str, str]]:
        """R√©cup√®re une ressource unique (ex: CSS)."""
        try:
            async with self.session.get(url, timeout=aiohttp.ClientTimeout(total=10)) as response:
                response.raise_for_status()
                content = await response.text(encoding='utf-8', errors='replace')
                return {'url': url, 'content': content}
        # CORRECTION 5: Gestion d'exceptions plus sp√©cifique
        except aiohttp.ClientError as e:
            logger.warning(f"Erreur HTTP lors de la r√©cup√©ration de {url}: {e}")
        except asyncio.TimeoutError:
            logger.warning(f"Timeout lors de la r√©cup√©ration de {url}.")
        except Exception as e:
            logger.error(f"Erreur inattendue lors de la r√©cup√©ration de {url}: {e}")
        return None

    def _hsl_to_hex(self, h: int, s_str: str, l_str: str, a: float = 1.0) -> str:
        """Convertit une couleur HSL(A) en code hexad√©cimal."""
        s = float(s_str.strip('%')) / 100.0
        l = float(l_str.strip('%')) / 100.0
        r, g, b = colorsys.hls_to_rgb(h / 360.0, l, s)

        # CORRECTION 6: Logique de blending alpha corrig√©e (sur fond blanc)
        if a < 1.0:
            r = r * a + 1.0 * (1 - a)
            g = g * a + 1.0 * (1 - a)
            b = b * a + 1.0 * (1 - a)

        return f"#{int(r*255):02x}{int(g*255):02x}{int(b*255):02x}"

    async def _extract_color_palette(self, all_css_content_list: List[str], soup: BeautifulSoup) -> List[str]:
        """Extraction de la palette de couleurs depuis le CSS et le HTML."""
        colors = set()
        text_to_scan = "\n".join(all_css_content_list)
        for element in soup.find_all(style=True):
            text_to_scan += ";" + element['style']

        # Hex
        colors.update(m.group(0).lower() for m in self._COLOR_HEX_PATTERN.finditer(text_to_scan))
        # RGB/RGBA
        for m in self._COLOR_RGB_PATTERN.finditer(text_to_scan):
            colors.add(webcolors.rgb_to_hex((int(m.group(1)), int(m.group(2)), int(m.group(3)))))
        for m in self._COLOR_RGBA_PATTERN.finditer(text_to_scan):
            r, g, b = int(m.group(1)), int(m.group(2)), int(m.group(3))
            a = float(m.group(4))
            if a >= 1.0:
                colors.add(webcolors.rgb_to_hex((r, g, b)))
            # Ignorer les couleurs transparentes pour la palette principale
        # HSL/HSLA
        for m in self._COLOR_HSL_PATTERN.finditer(text_to_scan):
            colors.add(self._hsl_to_hex(int(m.group(1)), m.group(2), m.group(3)))
        for m in self._COLOR_HSLA_PATTERN.finditer(text_to_scan):
            if float(m.group(4)) >= 1.0:
                colors.add(self._hsl_to_hex(int(m.group(1)), m.group(2), m.group(3), float(m.group(4))))
        # Noms de couleurs
        for m in self._COLOR_NAMES_PATTERN.finditer(text_to_scan):
            color_name = m.group(1).lower()
            # CORRECTION 7: Coh√©rence de l'attribut
            try:
                name = webcolors.hex_to_name(color_name, spec='css3')
                colors.add(name)
            except ValueError:
                # color_name n'est pas une couleur nomm√©e CSS3 ; ignorez ou g√©rez autrement
                pass
    
        # CORRECTION 19: Retourner une liste vide au lieu de None
        if not colors:
            return []

        # Clustering pour trouver les couleurs dominantes
        hex_colors_for_clustering = [webcolors.hex_to_rgb(c) for c in colors if len(c) == 7]
        if len(hex_colors_for_clustering) < 3:
            return sorted(list(colors))[:12]

        pixels = np.array(hex_colors_for_clustering)
        n_clusters = min(8, len(pixels))
        # CORRECTION 8: Param√®tre `n_init` g√©r√© automatiquement par les versions r√©centes
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto') # Ajout de n_init='auto'
        kmeans.fit(pixels)
        dominant_colors = [webcolors.rgb_to_hex(tuple(map(int, center))) for center in kmeans.cluster_centers_]
        return sorted(dominant_colors)

    async def _analyze_typography(self, all_css_content_list: List[str], soup: BeautifulSoup) -> Dict[str, Any]:
        """Analyse la typographie de la page."""
        # CORRECTION 35: Normalisation des noms de police
        fonts = set(
            family.strip().strip("'\"").lower()
            for css in all_css_content_list
            for match in self._FONT_FAMILY_PATTERN.finditer(css)
            for family in match.group(1).split(',')
        )

        # CORRECTION 24: Protection contre la division par z√©ro
        headings = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
        avg_heading_length = 0
        if headings:
            avg_heading_length = sum(len(h.get_text(strip=True)) for h in headings) / len(headings)

        return {
            'font_families': sorted(list(fonts)),
            'common_sizes': [], # L'extraction pr√©cise n√©cessite le rendu du DOM
            'avg_heading_length': round(avg_heading_length, 2)
        }

    # --- M√©thodes Stubs et Incompl√®tes ---
    # CORRECTION 13, 16, 40: Ajout de stubs pour les m√©thodes manquantes

    async def _extract_layout_patterns(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """(Stub) Extrait les motifs de mise en page."""
        return {'grid_system': 'Unknown', 'main_containers': []}

    async def _analyze_spacing_system(self, all_css_content_list: List[str], soup: BeautifulSoup) -> Dict[str, Any]:
        """(Stub) Analyse le syst√®me d'espacement."""
        return {'base_unit': 8, 'scale': [1, 2, 3, 4, 6]}

    async def _detect_visual_hierarchy(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """(Stub) D√©tecte la hi√©rarchie visuelle."""
        return {'heading_levels': len(soup.find_all(['h1', 'h2', 'h3']))}

    async def _extract_branding_elements(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """(Stub) Extrait les √©l√©ments de marque."""
        logo = soup.find('img', alt=re.compile(r'logo', re.I))
        return {'logo_found': bool(logo)}

    async def _analyze_responsive_design(self, all_css_content_list: List[str]) -> List[int]:
        """(Stub) Analyse le design responsive via les media queries."""
        # CORRECTION 37: Gestion simple des media queries
        breakpoints = set()
        for css in all_css_content_list:
            for match in re.finditer(r'@media\s*.*?(\d+)(px|em|rem)', css):
                breakpoints.add(int(match.group(1)))
        return sorted(list(breakpoints))

    def _extract_css_rules(self, all_css_content_list: List[str]) -> Dict[str, Any]:
        """(Stub) Extrait et nettoie les r√®gles CSS."""
        # CORRECTION 23, 39: Utilisation de tinycss2 avec gestion d'erreur
        rules = {}
        # Cette fonction reste un stub car une analyse compl√®te est complexe.
        return {'rule_count': sum(css.count('{') for css in all_css_content_list)}

    async def _extract_metadata_profile(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """Extrait les m√©tadonn√©es de la page."""
        title = soup.title.string if soup.title else "N/A"
        description = soup.find('meta', attrs={'name': 'description'})
        return {
            'title': title.strip(),
            'description': description['content'].strip() if description else "N/A"
        }

    async def _analyze_design_mood_with_ai(self, soup: BeautifulSoup, colors: List[str], typography: Dict) -> str:
        """(Stub) Analyse l'ambiance du design avec l'IA."""
        if not self.openai_client:
            return "neutral"
        # Le code d'appel √† l'API OpenAI irait ici.
        return "modern"

    def _calculate_analysis_confidence(self, analysis_data: Dict[str, Any]) -> float:
        """Calcule un score de confiance pour l'analyse HTML."""
        factors = {}
        # CORRECTION 10: Logique de confiance am√©lior√©e
        color_count = len(analysis_data.get('color_palette', []))
        if 3 <= color_count <= 10: factors['color_palette'] = 1.0
        elif color_count > 10: factors['color_palette'] = 0.7
        else: factors['color_palette'] = 0.5

        # ... autres facteurs ...

        if not factors: return 0.0
        return round(sum(factors.values()) / len(factors), 2)

    def _calculate_pdf_analysis_confidence(self, pdf_analysis_data: Dict[str, Any]) -> float:
        """Calcule un score de confiance pour l'analyse PDF."""
        factors = {}
        color_count = len(pdf_analysis_data.get('color_palette', []))
        if color_count > 2: factors['colors'] = 1.0
        else: factors['colors'] = 0.5

        if pdf_analysis_data.get('text_content', ''): factors['text'] = 1.0
        else: factors['text'] = 0.2

        if not factors: return 0.0
        return round(sum(factors.values()) / len(factors), 2)

    async def _analyze_pdf_document(self, pdf_data: bytes) -> Dict[str, Any]:
        """Analyse un document PDF pour en extraire les styles."""
        doc = None
        # CORRECTION 14, 18, 33: Gestion robuste des ressources avec try/finally et exceptions sp√©cifiques
        try:
            doc = fitz.open(stream=pdf_data, filetype="pdf")
            text_content = "".join(page.get_text() for page in doc)

            # CORRECTION 26: Optimisation - analyse d'un sous-ensemble de pages
            page_count = len(doc)
            pages_to_analyze = doc if page_count <= 5 else [doc[i] for i in range(5)]

            image_colors = set()
            image_count = 0
            for page in pages_to_analyze:
                for img_info in page.get_images(full=True):
                    image_count += 1
                    xref = img_info[0]
                    base_image = doc.extract_image(xref)
                    image_bytes = base_image["image"]

                    # CORRECTION 12: Gestion d'exception pour le traitement d'image
                    try:
                        img = Image.open(io.BytesIO(image_bytes))
                        if img.mode == 'RGBA': img = img.convert('RGB')
                        img.thumbnail((150, 150))
                        pixels = np.array(img).reshape(-1, 3)

                        n_clusters = min(5, len(np.unique(pixels, axis=0)))
                        if n_clusters > 1:
                            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto') # Ajout de n_init='auto'
                            kmeans.fit(pixels)
                            for center in kmeans.cluster_centers_:
                                image_colors.add(webcolors.rgb_to_hex(tuple(map(int, center))))
                    except Exception as e:
                        logger.warning(f"Impossible de traiter une image du PDF: {e}")

            return {
                'color_palette': sorted(list(image_colors)),
                'typography': {'font_families': sorted(list(set(font[3] for page in doc for font in page.get_fonts())))},
                'text_content': text_content,
                'page_count': page_count,
                'image_count': image_count,
            }
        except Exception as e:
            logger.error(f"Erreur lors de l'analyse du document PDF: {e}")
            raise ValueError("Le document PDF est peut-√™tre corrompu ou illisible.")
        finally:
            if doc:
                doc.close()

    # --- M√©thodes de Transfert de Style (Stubs) ---
    async def apply_style_transfer(self, target_html: str, fingerprint: StyleFingerprint) -> str:
        """(Stub) Applique une empreinte stylistique √† un HTML cible."""
        logger.info("Application du transfert de style (stub).")
        css_rules = self._generate_transfer_css(fingerprint)
        soup = BeautifulSoup(target_html, 'html.parser')

        # Ajouter les nouvelles r√®gles CSS
        new_style_tag = soup.new_tag('style')
        new_style_tag.string = css_rules
        if soup.head:
            soup.head.append(new_style_tag)
        else:
            soup.insert(0, soup.new_tag('head')) # Correction ici : cr√©ez le head s'il n'existe pas
            soup.head.append(new_style_tag) # Ajoutez le style apr√®s avoir cr√©√© le head


        # Appliquer des classes de transfert (logique √† impl√©menter)
        self._apply_transfer_classes(soup, fingerprint)

        return str(soup)

    def _generate_transfer_css(self, fingerprint: StyleFingerprint) -> str:
        """(Stub) G√©n√®re une feuille de style CSS √† partir d'une empreinte."""
        css = "/* --- Feuille de Style G√©n√©r√©e --- */\n"
        if fingerprint.color_palette:
            css += f":root {{\n    --primary-color: {fingerprint.color_palette[0]};\n}}\n"
        # ... plus de r√®gles ...
        return css

    def _apply_transfer_classes(self, soup: BeautifulSoup, fingerprint: StyleFingerprint):
        """(Stub) Applique des classes CSS aux √©l√©ments pour le transfert."""
        # Cette fonction n√©cessiterait une logique complexe pour mapper les styles
        # aux √©l√©ments s√©mantiques (ex: trouver le bouton principal et lui appliquer la couleur primaire).
        pass
async def main():
    """Fonction principale pour tester l'analyseur."""
    print("üöÄ D√©marrage de l'analyseur de style web...")
    
    # V√©rification des d√©pendances
    if not check_critical_dependencies():
        print("\n‚ùå Impossible de continuer sans les d√©pendances critiques")
        return
    
    # Configuration des cl√©s API
    ai_config = get_ai_config()
    
    # V√©rification et affichage du statut des cl√©s API
    openai_configured = bool(ai_config.get('openai_api_key'))
    anthropic_configured = bool(ai_config.get('anthropic_api_key'))
    
    print(f"\nüîë Configuration API:")
    print(f"   OpenAI: {'‚úÖ Configur√©' if openai_configured else '‚ùå Non configur√©'}")
    print(f"   Anthropic: {'‚úÖ Configur√©' if anthropic_configured else '‚ùå Non configur√©'}")
    
    if not openai_configured and not anthropic_configured:
        print("\n‚ö†Ô∏è  Aucune cl√© API configur√©e. L'analyse IA sera limit√©e.")
        print("Pour configurer :")
        print("   export OPENAI_API_KEY='sk-...'")
        print("   export ANTHROPIC_API_KEY='sk-ant-...'")

    # Test d'analyse
    target_url = "https://httpbin.org/html"  # URL de test simple
    
    try:
        async with WebStyleAnalyzer(ai_config) as analyzer:
            print(f"\nüîç Analyse de l'URL : {target_url}")
            fingerprint = await analyzer.analyze_reference_page(target_url, is_url=True, is_pdf=False)
            
            print("\n--- ‚úÖ Empreinte Stylistique ---")
            print(f"üé® Palette de couleurs ({len(fingerprint.color_palette)} couleurs): {fingerprint.color_palette[:5]}{'...' if len(fingerprint.color_palette) > 5 else ''}")
            print(f"üìù Polices trouv√©es: {len(fingerprint.typography.get('font_families', []))}")
            print(f"üé≠ Ambiance du design: {fingerprint.design_mood}")
            print(f"üìä Score de confiance: {fingerprint.confidence_score}")
            
            if fingerprint.metadata_profile:
                print(f"üìÑ Titre: {fingerprint.metadata_profile.get('title', 'N/A')[:50]}...")
            
            print(f"\nüìà Statistiques:")
            print(f"   - Breakpoints responsive: {len(fingerprint.responsive_breakpoints)}")
            print(f"   - R√®gles CSS d√©tect√©es: {fingerprint.css_rules.get('rule_count', 0)}")
            
    except (ValueError, ImportError) as e:
        print(f"\n‚ùå Erreur de configuration: {e}")
    except Exception as e:
        print(f"\nüí• Erreur inattendue: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    if WEB_ANALYSIS_AVAILABLE:
        asyncio.run(main())
    else:
        print("‚ùå Impossible de d√©marrer sans les d√©pendances critiques")
        print(INSTALLATION_INSTRUCTIONS)

